# ğŸ“š Document Processing Pipeline ğŸš€

This project provides a comprehensive solution for processing document collections, extracting relevant information, and generating summaries. It's designed to handle PDF documents, extract text, identify key sections, and provide concise summaries based on a given query. This README provides a detailed overview of the project, its features, setup instructions, and how to contribute.

## âœ¨ Key Features

- **Directory Traversal:** Automatically scans specified directories for document collections. ğŸ“‚
- **Configuration Loading:** Loads processing configurations from JSON files. âš™ï¸
- **PDF Extraction:** Extracts text blocks from PDF documents. ğŸ“„
- **Semantic Embedding:** Generates text embeddings using Sentence Transformers. ğŸ§ 
- **Section Ranking:** Ranks document sections based on semantic similarity to a query. ğŸ¥‡
- **Text Summarization:** Summarizes the top-ranked sections using the TextRank algorithm. ğŸ“
- **Output Generation:** Creates JSON output files containing metadata, ranked sections, and summaries. ğŸ“¤
- **Heading Classification:** Identifies and classifies headings within documents for improved structure. ğŸ”–
- **Outline Extraction:** Extracts existing outlines from PDFs or generates them from classified headings. ğŸŒ³

## ğŸ› ï¸ Tech Stack

| Category    | Technology                      | Version (if specified) | Description                                                                 |
|-------------|---------------------------------|------------------------|-----------------------------------------------------------------------------|
| **Language** | Python                          | 3.x                    | The primary programming language used for the project.                       |
| **PDF Parsing** | PyMuPDF (fitz)                  | N/A                    | Used for opening, parsing, and extracting text from PDF files.             |
| **NLP**       | Sentence Transformers           | 2.2.2                  | Used for generating text embeddings.                                        |
|             | scikit-learn                    | N/A                    | Used for calculating cosine similarity between embeddings.                  |
|             | sumy                            | N/A                    | Used for text summarization using the TextRank algorithm.                   |
|             | nltk                            | N/A                    | Natural Language Toolkit, used by Sumy for text processing.                |
| **Data Handling** | json                            | N/A                    | Used for loading and saving JSON data.                                    |
| **Other**     | os                              | N/A                    | Used for interacting with the operating system (file system operations). |
|             | re                              | N/A                    | Used for regular expression operations (text cleaning).                    |
|             | datetime                        | N/A                    | Used for generating timestamps in the output.                               |
| **Hugging Face** | huggingface_hub | 0.14.1 | Used for accessing pre-trained models. |

## ğŸ“¦ Getting Started

### Prerequisites

- Python 3.6+
- `pip` package installer

### Installation

1.  **Clone the repository:**

    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  **Create a virtual environment (recommended):**

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Linux/macOS
    venv\Scripts\activate.bat  # On Windows
    ```

3.  **Install the dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

### Running Locally

1.  **Place your PDF documents in the `Challenge_1a/input` directory.**
2.  **Ensure you have a `challenge1b_input.json` file in each collection directory within the `Collections` directory.**
3.  **Run the main script:**

    For Challenge 1a:

    ```bash
    python Challenge_1a/src/extractor.py
    ```

    For Challenge 1b:

    ```bash
    python Challenge_1b/main.py
    ```

4.  **The output JSON files will be generated in the `Challenge_1a/output` directory for Challenge 1a and within each collection directory for Challenge 1b.**

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Challenge_1a/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ extractor.py       # Main entry point for Challenge 1a
â”‚   â”‚   â”œâ”€â”€ heading_utils.py   # Heading classification utilities
â”‚   â”‚   â”œâ”€â”€ pdf_utils.py       # PDF extraction utilities
â”‚   â”œâ”€â”€ input/               # Directory for input PDF files (Challenge 1a)
â”‚   â”œâ”€â”€ output/              # Directory for output JSON files (Challenge 1a)
â”œâ”€â”€ Challenge_1b/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ embedder.py      # Text embedding and ranking
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py    # PDF parsing utilities
â”‚   â”‚   â”œâ”€â”€ processor.py     # Core processing logic for Challenge 1b
â”‚   â”‚   â”œâ”€â”€ summarizer.py    # Text summarization utilities
â”‚   â”œâ”€â”€ main.py              # Main entry point for Challenge 1b
â”œâ”€â”€ Collections/           # Directory containing document collections for Challenge 1b
â”‚   â”œâ”€â”€ Collection_1/      # Example collection directory
â”‚   â”‚   â”œâ”€â”€ challenge1b_input.json # Input configuration file
â”‚   â”‚   â”œâ”€â”€ document1.pdf
â”‚   â”‚   â”œâ”€â”€ document2.pdf
â”œâ”€â”€ requirements.txt       # Project dependencies
â””â”€â”€ README.md              # This file
```




